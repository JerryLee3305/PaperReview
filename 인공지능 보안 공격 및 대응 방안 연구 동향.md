# 인공지능 보안 공격 및 대응 방안 연구 동향

### 류권상, 최대선 -2020


1. 서론
- 학습 단계에서 발생할 수 있는 보안 취약점 - Poisoning 공격, Backdoor 공격
- 활용 단계에서 발생할 수 있는 보안 취약점 - adversarial attack, model inversion, membersip inference

방어 기술 최신 연구 살펴볼 예정

1. 공격 기술
    1. backdoor 공격
    
    전이 학습을 통한 backdoor 공격 기술
    
    전이 학습 : 대용량 데이터 셋으로 학습된 모델(Teacher model)을 자신의 데이터 셋으로 재학습(student model)
    
    공격자가 선생 모델에 트리거(Trigger)를 삽입해 목표 클래스(Target Class)로 오분류하게끔 만듦 ⇒ 해당 선생 모델을 가지고 학생 모델 학습 시, 학생 모델에 트리거가 삽입된 데이터를 입력 시켜 오작동 유발
    
    b. 적대적 공격 (adversarial attack)
    
    이미지에서 특정 부분만 변조해 모델 오인식 하도록 함
    
    이미지 왜곡 줄이기 위한 목적 함수 + 오익식 하는 최적 노이즈 값 찾는 목적 함수 * 반영 비율
    
    c. model inversion
    
    공격자가 상위 m개의 예측 확률만 이용하여 모델 학습에 사용된 데이터를 추출할 수 있는 방법
    
![image](https://user-images.githubusercontent.com/108413432/229329211-6107d7fe-21a6-4cb2-a1cd-8c656258166e.png)

    F는 모델의 예측 확률 벡터
    
    trunc()는 상위 m개를 제외한 나머지 확률 값을 0으로 만드는 함수
    
    R()은 원본과 추출된 것의 차이로 유크리디안 거리 사용
    
    d. membership inference
    
    적대적 공격을 방어하기 위한 적대적 학습된 모델이 membership inference 공격에 더 취약
    
    적대적 학습은 모델의 과적합(overfitting) 문제가 발생하기에 공격 성공률이 더 높음
    
2. 방어 기술
    1. 적대적 공격 방어
        - Feature Sqeezing
        
        8 bits 이미지의 color-bit 줄이고, 적대적 예제에 포함된 노이즈가 가지는 이미지 줄이기 위해 Median Smoothing 과 Non-local Smoothing 사용 ⇒ 각 모델 출력 값 차이를 계산해 차이가 크면 적대적 예제로 탐지, 아니면 정상으로 판단
        
        - NPR(Neural Representaion Purifier)
        
        GAN 구조, 적대적 예제에 포함된 노이즈를 제거하기 위한 네트워크 ⇒ 노이즈 제거된 적대적 예제와 정상이미지 구분 하는 Critic Network 사용
        
![image](https://user-images.githubusercontent.com/108413432/229329216-f3ea6a03-254a-49d2-9489-a6dcdfe71577.png)

    2. membership inference 방어
        - MemGuard
        
        방어자 스스로 공격 모델 생성해 자신의 공격 모델을 속이도록 모델 예측값에 노이즈를 추가하여 안전성을 높임
        
    3. 프라이버시 보호
        - Fawkes
        
        사용자 얼굴 이미지에 다른 사람 특징 삽입
        
        얼굴 이미지에 사람이 알아차릴 수 없는 노이지를 삽입하여 정상 이미지 입력하면 잘못된 결과를 예측하도록 함.
